Project Guidline:

Paper: https://arxiv.org/abs/1603.08511 \citep{Zhang.2016}
- we will only look at their best results
- classes sind farbwerte
- 10x10 große bins: gucke wie oft die einzelnen farbwerte in dem bins vorkommen
-> das benutzen um zu reweighten (das haben wir nicht gemacht - next steps?)

1. Dataset suchen (mehrere Vorschläge willkommen)
!!! dataset quellen sind bis jetzt nicht in der bibfile, falls wir die doch irgendwo citen, sagt bescheid!!!
-> cifar(zu wenig klassen), und verschiedene Varianten von imagenet auf dem
  Server
-> im paper PASCAL dataset (http://host.robots.ox.ac.uk/pascal/VOC/)
  und Imagenet
-> Pascal: 20 classes. The train/val data has 11,530 images containing 27,450
  ROI annotated objects and 6,929 segmentation.
  Datasets for classification, detection and person layout are
  the same as VOC2011.
-> vergleich mit SUN dataset (https://groups.csail.mit.edu/vision/SUN/)
-> SUN: large variety of environmental scenes, places and the objects within
  SUN2012: 16,873 images, Also available in PASCAL format (ist das also nur ein format?)
- "Though our model was trained on object-centric ImageNet dataset, we demonstrate
that it nonetheless remains effective for photos from the scene-centric SUN dataset" (p. 23)
--> ImageNet 2012 chosen, 1000 classes with 1300 images each, for training we used 5 image from each class = 5000 images, more was not possible because of time and computational power constraints

2. Bilder in lab und greyscale
2.1 große Mengen an Bildern (1 Mio) handeln (lade problem?)
- skript, dass das datenset generiert (1000 bilder pro 1000 klassen ist zu groß)
- ziel mindestens 100.000 (das haben wir leider nicht)
- als numpy oder image data?

2.2 kleineres datenset zum debuggen etc erstellen - selbe klasse 100 bilder
Z:\net\projects\scratch\winter\valid_until_31_July_2020\asparagus\LabelFiles\colorize_images

2.2 preprocessing: cropping, labspace umwandeln (statt RGB) - sophia
- in lab space umwandeln: mit cv2, angelehnt an diese Funktion: https://medium.com/@halmubarak/changing-color-space-hsv-lab-while-reading-from-directory-in-keras-c8ca243e2d57 \citep{Almubarak.2018}
- alternative, die wir aber nicht benutzt haben wäre in scikit https://scikit-image.org/docs/dev/api/skimage.color.html#skimage.color.rgb2lab \citep{vanderWalt2014} - über einige umwege habe ich hier ein paper gefunden, dass man citen soll, falls man das project useful findet.. alternative wäre \citep{scikit-image}
- labspace ist ein colorspace, luminanca und dann farbe encoden
-> d.h. erste layer ist greyscale, channel ab ist target
- the preprocessing is included in the data generator (what is a generator, how does it generally work, how does our own implementation work)

2.3. trenne datenset in l und ab
- cropping of L und ab mit dieser Funktion tf.unstack(lab, axis=2) https://github.com/xahidbuffon/rgb-lab-conv/blob/master/rgb_lab_formulation.py \citep{buffon.2018} (siehe preprocessor/generator)

2.4 loss function schreiben (caffe) - from original paper versuchen anzupassen? - malin
- annealed mean?
- reweightender einzelnen bins (nicht gemacht)

3. lege report als texfile an (maren)

4. layer nachbauen
- do we use keras, tf1 or tf2(with keras), how can we use what code?
  blogpost on difference: https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/ \citep{Rosebrock.2019}
  some info found in this book:  (specifically p.109 and surrounding ones for model and keras writing for preprocessing of dataGenerator) https://books.google.de/books?id=l86PDwAAQBAJ&pg=PA109&lpg=PA109&dq=keras.preprocessing.image.ImageDataGenerator(featurewise_center%3DFalse,+samplewise_center%3DFalse,+featurewise_std_normalization%3DFalse,+samplewise_std_normalization%3DFalse,+zca_whitening%3DFalse,+zca_epsilon%3D1e-06,+rotation_range%3D0,+width_shift_range%3D0.0,+height_shift_range%3D0.0,+brightness_range%3DNone,+shear_range%3D0.0,+zoom_range%3D0.0,+channel_shift_range%3D0.0,+fill_mode%3D%27nearest%27,+cval%3D0.0,+horizontal_flip%3DFalse,+vertical_flip%3DFalse,+rescale%3DNone,+preprocessing_function%3DNone,+data_format%3D%27channels_last%27,+validation_split%3D0.0,+interpolation_order%3D1,+dtype%3D%27float32%27)+in+tf+2.0&source=bl&ots=4PaPUc1vta&sig=ACfU3U2jvCN8UW4dirz_NtwMcQGAq4Gugg&hl=de&sa=X&ved=2ahUKEwjyx4n10tHnAhXCfZoKHfAjBikQ6AEwAXoECAoQAQ#v=onepage&q=keras.preprocessing.image.ImageDataGenerator(featurewise_center%3DFalse%2C%20samplewise_center%3DFalse%2C%20featurewise_std_normalization%3DFalse%2C%20samplewise_std_normalization%3DFalse%2C%20zca_whitening%3DFalse%2C%20zca_epsilon%3D1e-06%2C%20rotation_range%3D0%2C%20width_shift_range%3D0.0%2C%20height_shift_range%3D0.0%2C%20brightness_range%3DNone%2C%20shear_range%3D0.0%2C%20zoom_range%3D0.0%2C%20channel_shift_range%3D0.0%2C%20fill_mode%3D'nearest'%2C%20cval%3D0.0%2C%20horizontal_flip%3DFalse%2C%20vertical_flip%3DFalse%2C%20rescale%3DNone%2C%20preprocessing_function%3DNone%2C%20data_format%3D'channels_last'%2C%20validation_split%3D0.0%2C%20interpolation_order%3D1%2C%20dtype%3D'float32')%20in%20tf%202.0&f=false
  \citep{holdroyd2019}
- conv layer same padding in tensorflow und in dem model ist pad: 1 steht und in caffe
-> für conv layer 5 und 6 benutzten die caffe pad: 2: um das umzusetzen müssten wir eine neue pad funktion schreiben
daher weichen wir erstmal vom code ab, falls das nicht funktioniert (hier in irgendeinem ordner https://github.com/richzhang/colorization/blob/master/interactive-deep-colorization/data/lab_gamut.py) - das ist das github vom ersten artikel, daher einfach das paper citen?


5. training

6. exp.

7. visualisierung der results

8. Blogpost fertig schreiben

##########################################################################################################

REPORT:
1. Introduction/Motivation
Based on the paper \emph{Colorful Image Colorization} \citep{Zhang.2016}
 this project aims to reimplement a similar artificial neuronal net that transforms grayscale images
 into colorful pictures.\\
This involves first creating a dataset based on pictures that are converted into the CIELAB colorspace
 (Lab), such that the first channel \enquote{L} can be considered as input as it is grayscale whereas the
\enquote{a} and \enquote{b} channel are the target labels to be predicted. Thus, the problem can be
 handled as classification task. In the second step, the aim was to closely rebuilt the layers of the
 original model (which used \enquote{caffe} \citep{jia2014caffe}) using tensorflow 2.0 (richtige version?).
 Other project have trained convolutional neural networks (CNNs) on the color prediction problem
 before (e.g. \cite{Cheng_2015}, \cite{Dahl.2016}). The training data is easily available which enables
 training on large datasets. Problem about previous approaches is that they try to predict the ground truth
 rather than a possible truth. A conservative loss function tries to minimize Euclidean error between
 estimate and ground truth. As objects can have various plausible colors, these predictions are multimodal.
 Thus, the approach of Zhang et al. (2016) innovates a loss function that predicts plausible colors for
 pixels, rather than the original color \citep{Zhang.2016}.\\
\\

- we want to built a network that can colorize images
- we want to built a network that can colorize images\\
- this project aims to produce colorful images, given a greyscale picture.\\
- transforming greyscale into plausible colors is an easy task for humans\\
- We see a greyscale picture showing a woman playing volleyball at the beach. As we can recognize the scene and the form and relate to it. The sand is yellow, the sea is blue and the ball is white.\\
- But coloring it in life would be a much more difficult task. As we also need to consider different textures, shades and so on. Seeing and imagining things does not make people a proper painter.\\
- Surface structure and the semantics of the scene are necessary to validly color images.\\
- this project aims does not aim to generate the true color for pictures but at least a good and prediction.\\
- aus dem paper: model enough of the statistical dependencies between semantics and the textures of greyscale images and their color versions in order to produce visually compelling results.\\
- haben wir noch relaated work???

2. Important background knowledge (including reference to most relevant publications)
- was ist colorization\\
- what is CNN for image colorization?\\
- wie ist der status quo: -- related work\\
- was sind die schwierigkeiten von den colorization sache (sepia ding)\\
- was hat Zhang gemacht - was hat er besser gemacht\\

- We started looking at this paper:
https://arxiv.org/abs/1603.08511 \citep{Zhang.2016}
- at their best solution
- vielleicht andere color problems und wo deren schwachstellen sind?
- und dann sozusagen, was das coole an dem paper ist, das wir haben.

3. The model and the experiment (MAIN PART). This part should feature code.
- hier nur erklären was wir machen\\
- layers as in the paper
- 8 groups of conv layers with 2 or 3 con layers, relu activateion and batch normalization at the end, same padding, sometimes strides (nicht so detailiert sondern auf code verweisen)
- the decoding layer is conv2Dtranspose to 313 neurons as in paper (nochmal nachlesen warim 313)
- sgd optimizer with learning rate 0.001 and momentom 0.9
- MSE loss, should be replaced by costum loss, taking the mean loss leads to sepia-colored images (theory behind this)
- 10 epochs, batch size 20, 5000 images ---> should be more/longer but time constraints/computational power
- evalutation of results with loss values and visiual inspection of results --> make predictions for unseen images
- idea: achieve better results by reducing the number of colors, didn't help

3.1 Dataset
- loading large amount of data
https://machinelearningmastery.com/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras/ \citep{Brownlee.2019}
- we decided to take Imagenet as we have it on the IKW server and it provides a lot of images
- Data generator
  There are inbuilt data generators for large amount of data and also for images specifically, but we do not want to classify images (input:    image, output: label) but colorize them. Therefore, we wrote our own image generator.
  Images are loaded and transformed from BGR to LAB color space, the first layer (luminance) which displayes most of the structure, is  separted from the other two and used as the input, the ab layers, which encode the color information of the images, are used as the target  of the model. Slightly different version to generate the test data, because they have a different folder structure and there the whole batch  should be taken at once.

3.2 Preprocessing
- image preprocessing documentation
https://keras.io/preprocessing/image/#imagedatagenerator-class \citep{chollet2015keras}
(preprocessing happens in generator)

3.3 Modelstructure
- besser hier layer und loss-function
- built the same layers aas Zhang et al, but no probability distribution\\
-networkstructure and design\\
- MSE loss should be costumized loss with weighting by occurance of the color values

4. Visualization and discussion of your results.
- original and predicted in comparison (side by side) maybe with their results too to see how good we wanted to get?
- loss curve

4.1 Training

4.2 Testing
